{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Successive Halving.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcgcJCCQZLNw"
      },
      "source": [
        "#Please run the first 3 cells to have all things to work\r\n",
        "\r\n",
        "!pip install stable-baselines\r\n",
        "#!pip install --upgrade pip\r\n",
        "!pip install gym[box2d]\r\n",
        "!pip install -c conda-forge pyglet\r\n",
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\r\n",
        "!apt-get install swig cmake libopenmpi-dev zlib1g-dev xvfb x11-utils ffmpeg -qq #remove -qq for full output\r\n",
        "!pip install stable-baselines[mpi] box2d box2d-kengz pyvirtualdisplay pyglet==1.3.1 --quiet #remove --quiet for full output \r\n",
        "# Stable Baselines only supports tensorflow 1.x for now\r\n",
        "#%tensorflow_version 1.x\r\n",
        "\r\n",
        "import imageio\r\n",
        "import base64\r\n",
        "import IPython\r\n",
        "import PIL.Image\r\n",
        "import pyvirtualdisplay\r\n",
        "#Video stuff \r\n",
        "from pathlib import Path\r\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCUNeA5xPoPi"
      },
      "source": [
        "#!sudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev\r\n",
        "!pip install stable-baselines[mpi]\r\n",
        "\r\n",
        "%tensorflow_version 1.x\r\n",
        "#import tensorflow as tf\r\n",
        "\r\n",
        "#!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\r\n",
        "!pip install stable-baselines[mpi]==2.10.0\r\n",
        "\r\n",
        "#!pip install gym\r\n",
        "#!pip install gym[atari]\r\n",
        "\r\n",
        "#import os\r\n",
        "#os.kill(os.getpid(), 9)\r\n",
        "\r\n",
        "import stable_baselines\r\n",
        "stable_baselines.__version__\r\n",
        "\r\n",
        "import gym\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxmREapgY-wF"
      },
      "source": [
        "#Load the TensorBoard notebook extension\r\n",
        "%load_ext tensorboard\r\n",
        "import tensorflow as tf\r\n",
        "import datetime, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtD2SoWgXF2c"
      },
      "source": [
        "#use it when you need to remove old log values and show the new trained one\r\n",
        "#!rm -r ppo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcDQoWNDZr8F"
      },
      "source": [
        "# CartPole"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WplKF0OPr4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15352488-cf86-477a-ff8e-ece12ed66bc5"
      },
      "source": [
        "%tensorflow_version 1.x\r\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n",
        "\r\n",
        "\r\n",
        "try:\r\n",
        "  import optuna\r\n",
        "except ImportError:\r\n",
        "  !pip install optuna\r\n",
        "  import optuna\r\n",
        "import gym\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from stable_baselines import PPO2, DQN\r\n",
        "from stable_baselines.common.evaluation import evaluate_policy\r\n",
        "from stable_baselines.common.cmd_util import make_vec_env\r\n",
        "\r\n",
        "# https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/5_custom_gym_env.ipynb\r\n",
        "#from custom_env import GoLeftEnv\r\n",
        "\r\n",
        "def optimize_ppo2(trial):\r\n",
        "    \"\"\" Learning hyperparamters we want to optimise\"\"\"\r\n",
        "    return {\r\n",
        "        'n_steps': int(trial.suggest_loguniform('n_steps', 16, 2048)),\r\n",
        "        'gamma': trial.suggest_loguniform('gamma', 0.9, 0.9999),\r\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1.),\r\n",
        "        'ent_coef': trial.suggest_loguniform('ent_coef', 1e-8, 1e-1),\r\n",
        "        'cliprange': trial.suggest_uniform('cliprange', 0.1, 0.4),\r\n",
        "        'noptepochs': int(trial.suggest_loguniform('noptepochs', 1, 48)),\r\n",
        "        'lam': trial.suggest_uniform('lam', 0.8, 1.),\r\n",
        "        #'capacity': int(trial.suggest_loguniform('n_steps', 1, 100))\r\n",
        "    }\r\n",
        "\r\n",
        "\r\n",
        "def optimize_agent(trial):\r\n",
        "    \"\"\" Train the model and optimize\r\n",
        "        Optuna maximises the negative log likelihood, so we\r\n",
        "        need to negate the reward here\r\n",
        "    \"\"\"\r\n",
        "    # Create log dir\r\n",
        "    #log_dir = \"tmp/\"\r\n",
        "    #os.makedirs(log_dir, exist_ok=True)\r\n",
        "\r\n",
        "    env_name='CartPole-v1'\r\n",
        "    #env_name='LunarLander-v2'\r\n",
        "    #env_name='MountainCar-v0'\r\n",
        "    #env_name='Acrobot-v1'\r\n",
        "    model_params = optimize_ppo2(trial)\r\n",
        "    #env = make_vec_env(lambda: gym.make(env_name), n_envs=16, seed=0)\r\n",
        "    env = gym.make(env_name)\r\n",
        "    \r\n",
        "    model = PPO2('MlpPolicy', env, verbose=0, nminibatches=1, **model_params,\r\n",
        "                 tensorboard_log=\"./cartPole/\")\r\n",
        "    #model = PPO('MlpPolicy', env, verbose=0, **model_params)\r\n",
        "    \r\n",
        "    time_steps=20000\r\n",
        "    model.learn(total_timesteps=int(time_steps))\r\n",
        "\r\n",
        "\r\n",
        "    mean_reward, _ = evaluate_policy(model, gym.make(env_name), n_eval_episodes=10)\r\n",
        "    #print(model.)\r\n",
        "    return mean_reward\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    study = optuna.create_study(direction=\"maximize\",pruner=optuna.pruners.SuccessiveHalvingPruner())\r\n",
        "    try:\r\n",
        "        study.optimize(optimize_agent, n_trials=5, n_jobs=4,)\r\n",
        "        #%tensorboard --logdir ./ppo/\r\n",
        "    except KeyboardInterrupt:\r\n",
        "        print('Interrupted by keyboard.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-14 00:44:43,972]\u001b[0m A new study created in memory with name: no-name-257673d5-dcec-4651-bed9-6f49b488ba1b\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:46:31,974]\u001b[0m Trial 1 finished with value: 71.4 and parameters: {'n_steps': 606.2417149318093, 'gamma': 0.9838655933179025, 'learning_rate': 0.11311189551565551, 'ent_coef': 2.2419651672376814e-05, 'cliprange': 0.12379496248365413, 'noptepochs': 2.6720941917209076, 'lam': 0.9365721648673035}. Best is trial 1 with value: 71.4.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:46:33,417]\u001b[0m Trial 2 finished with value: 142.6 and parameters: {'n_steps': 1163.8226312860334, 'gamma': 0.9773512543701407, 'learning_rate': 0.01732035525868406, 'ent_coef': 0.003189465265855713, 'cliprange': 0.3468753061306529, 'noptepochs': 1.3540944866807738, 'lam': 0.9021292541166438}. Best is trial 2 with value: 142.6.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:46:41,217]\u001b[0m Trial 0 finished with value: 245.7 and parameters: {'n_steps': 1076.3789033095024, 'gamma': 0.9332154594191684, 'learning_rate': 0.003892845648900843, 'ent_coef': 0.028245591543255167, 'cliprange': 0.20527629366785313, 'noptepochs': 36.52721976678437, 'lam': 0.8679772188443583}. Best is trial 0 with value: 245.7.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:46:44,570]\u001b[0m Trial 3 finished with value: 331.8 and parameters: {'n_steps': 100.86652632606754, 'gamma': 0.9531120345744891, 'learning_rate': 0.00023712701587975135, 'ent_coef': 1.900894367795368e-08, 'cliprange': 0.12096254294983058, 'noptepochs': 2.9366218283107135, 'lam': 0.9686072304768809}. Best is trial 3 with value: 331.8.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:48:06,985]\u001b[0m Trial 4 finished with value: 9.5 and parameters: {'n_steps': 31.490055778105315, 'gamma': 0.9869544867408414, 'learning_rate': 0.17529547152260339, 'ent_coef': 0.0047539028932711115, 'cliprange': 0.33439332007252864, 'noptepochs': 17.009185577630518, 'lam': 0.9005359224310248}. Best is trial 3 with value: 331.8.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM1P5lIhZfnO"
      },
      "source": [
        "#Run cartpole\r\n",
        "%tensorboard --logdir ./cartPole/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI1-N00hcX4V"
      },
      "source": [
        "# LunarLander"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raEDEnZJZxee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6f351a-33e8-464d-c68f-176a86c41296"
      },
      "source": [
        "def optimize_agent(trial):\r\n",
        "    \"\"\" Train the model and optimize\r\n",
        "        Optuna maximises the negative log likelihood, so we\r\n",
        "        need to negate the reward here\r\n",
        "    \"\"\"\r\n",
        "    # Create log dir\r\n",
        "    #log_dir = \"tmp/\"\r\n",
        "    #os.makedirs(log_dir, exist_ok=True)\r\n",
        "\r\n",
        "    #env_name='CartPole-v1'\r\n",
        "    env_name='LunarLander-v2'\r\n",
        "    #env_name='MountainCar-v0'\r\n",
        "    #env_name='Acrobot-v1'\r\n",
        "    model_params = optimize_ppo2(trial)\r\n",
        "    #env = make_vec_env(lambda: gym.make(env_name), n_envs=16, seed=0)\r\n",
        "    env = gym.make(env_name)\r\n",
        "    \r\n",
        "    model = PPO2('MlpPolicy', env, verbose=0, nminibatches=1, **model_params,\r\n",
        "                 tensorboard_log=\"./lunarlander/\")\r\n",
        "    #model = PPO('MlpPolicy', env, verbose=0, **model_params)\r\n",
        "    \r\n",
        "    time_steps=20000\r\n",
        "    model.learn(total_timesteps=int(time_steps))\r\n",
        "\r\n",
        "\r\n",
        "    mean_reward, _ = evaluate_policy(model, gym.make(env_name), n_eval_episodes=10)\r\n",
        "    #print(model.)\r\n",
        "    return mean_reward\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    study = optuna.create_study(direction=\"maximize\",pruner=optuna.pruners.SuccessiveHalvingPruner())\r\n",
        "    try:\r\n",
        "        study.optimize(optimize_agent, n_trials=5, n_jobs=4,)\r\n",
        "        #%tensorboard --logdir ./lunarlander/\r\n",
        "    except KeyboardInterrupt:\r\n",
        "        print('Interrupted by keyboard.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-14 00:49:36,228]\u001b[0m A new study created in memory with name: no-name-6810605e-bdf3-4f04-989f-932b23b7bbe2\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:49:39,943]\u001b[0m Trial 2 finished with value: -105.71759627572624 and parameters: {'n_steps': 845.7774711919889, 'gamma': 0.9384890276983301, 'learning_rate': 0.0005456317266364103, 'ent_coef': 6.446493675195813e-05, 'cliprange': 0.39353368350716467, 'noptepochs': 1.8627910086771269, 'lam': 0.9038914894837258}. Best is trial 2 with value: -105.71759627572624.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:49:41,083]\u001b[0m Trial 1 finished with value: -841.885245605856 and parameters: {'n_steps': 1036.7215990234245, 'gamma': 0.9937370598806847, 'learning_rate': 0.003544362396638511, 'ent_coef': 1.974202717412899e-06, 'cliprange': 0.21890699218422788, 'noptepochs': 8.899961938379576, 'lam': 0.8711678533635603}. Best is trial 2 with value: -105.71759627572624.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:49:54,038]\u001b[0m Trial 3 finished with value: -541.6870796397176 and parameters: {'n_steps': 17.91650006366898, 'gamma': 0.9158441609057847, 'learning_rate': 0.00019246623607544058, 'ent_coef': 1.537363990274919e-08, 'cliprange': 0.3302038611762159, 'noptepochs': 3.9784303996933152, 'lam': 0.9144427312972981}. Best is trial 2 with value: -105.71759627572624.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:49:55,162]\u001b[0m Trial 0 finished with value: -601.4570110840424 and parameters: {'n_steps': 78.43948254407361, 'gamma': 0.9260452875359302, 'learning_rate': 0.010458203381039477, 'ent_coef': 0.0009563312531520556, 'cliprange': 0.281188702283935, 'noptepochs': 16.866347752443204, 'lam': 0.9618706656517064}. Best is trial 2 with value: -105.71759627572624.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:52:17,484]\u001b[0m Trial 2 finished with value: -279.42397549092556 and parameters: {'n_steps': 121.86180084360836, 'gamma': 0.9271356562805142, 'learning_rate': 0.05897627168146267, 'ent_coef': 1.0124987577055766e-08, 'cliprange': 0.2645682304283127, 'noptepochs': 2.2222385890696423, 'lam': 0.8014991041823186}. Best is trial 2 with value: -279.42397549092556.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:53:23,693]\u001b[0m Trial 3 finished with value: -613.3977672761375 and parameters: {'n_steps': 41.96998663711965, 'gamma': 0.9237897757362532, 'learning_rate': 0.48534502618863895, 'ent_coef': 1.7363907476986334e-06, 'cliprange': 0.28662611355386747, 'noptepochs': 10.727641153034766, 'lam': 0.8546027448905291}. Best is trial 2 with value: -279.42397549092556.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:54:38,794]\u001b[0m Trial 0 finished with value: -4616.869157540923 and parameters: {'n_steps': 167.5013411168049, 'gamma': 0.9248173848639972, 'learning_rate': 0.00013219281972325742, 'ent_coef': 1.6586014218062513e-08, 'cliprange': 0.24359396107612238, 'noptepochs': 33.72699726803265, 'lam': 0.833958277267182}. Best is trial 2 with value: -279.42397549092556.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:55:20,285]\u001b[0m Trial 1 finished with value: -212.25969703441538 and parameters: {'n_steps': 26.27040914991105, 'gamma': 0.985024480872244, 'learning_rate': 0.0055292490846104506, 'ent_coef': 1.630717383053732e-07, 'cliprange': 0.22163610826244734, 'noptepochs': 17.98594758341847, 'lam': 0.872501835553112}. Best is trial 1 with value: -212.25969703441538.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 00:55:53,160]\u001b[0m Trial 4 finished with value: -114.32780185928107 and parameters: {'n_steps': 20.805242169089926, 'gamma': 0.9400598384515235, 'learning_rate': 0.0002540515749972728, 'ent_coef': 0.00014043182982279376, 'cliprange': 0.23015030987308258, 'noptepochs': 10.01261920606609, 'lam': 0.8894111563065004}. Best is trial 4 with value: -114.32780185928107.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKb6sjo5cgRh"
      },
      "source": [
        "#Run lunarLander\r\n",
        "%tensorboard --logdir ./lunarlander/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy0Fx06ycjMu"
      },
      "source": [
        "# MountainCar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muxUEuWQckBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1611816-b47b-425d-bcb4-384c513d45d6"
      },
      "source": [
        "def optimize_agent(trial):\r\n",
        "    \"\"\" Train the model and optimize\r\n",
        "        Optuna maximises the negative log likelihood, so we\r\n",
        "        need to negate the reward here\r\n",
        "    \"\"\"\r\n",
        "    # Create log dir\r\n",
        "    #log_dir = \"tmp/\"\r\n",
        "    #os.makedirs(log_dir, exist_ok=True)\r\n",
        "\r\n",
        "    #env_name='CartPole-v1'\r\n",
        "    #env_name='LunarLander-v2'\r\n",
        "    env_name='MountainCar-v0'\r\n",
        "    #env_name='Acrobot-v1'\r\n",
        "    model_params = optimize_ppo2(trial)\r\n",
        "    #env = make_vec_env(lambda: gym.make(env_name), n_envs=16, seed=0)\r\n",
        "    env = gym.make(env_name)\r\n",
        "    \r\n",
        "    model = PPO2('MlpPolicy', env, verbose=0, nminibatches=1, **model_params,\r\n",
        "                 tensorboard_log=\"./MountainCar/\")\r\n",
        "    #model = PPO('MlpPolicy', env, verbose=0, **model_params)\r\n",
        "    \r\n",
        "    time_steps=20000\r\n",
        "    model.learn(total_timesteps=int(time_steps))\r\n",
        "\r\n",
        "\r\n",
        "    mean_reward, _ = evaluate_policy(model, gym.make(env_name), n_eval_episodes=10)\r\n",
        "    #print(model.)\r\n",
        "    return mean_reward\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    study = optuna.create_study(direction=\"maximize\",pruner=optuna.pruners.SuccessiveHalvingPruner())\r\n",
        "    try:\r\n",
        "        study.optimize(optimize_agent, n_trials=5, n_jobs=4,)\r\n",
        "        #%tensorboard --logdir ./MountainCar/\r\n",
        "    except KeyboardInterrupt:\r\n",
        "        print('Interrupted by keyboard.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-14 00:59:11,363]\u001b[0m A new study created in memory with name: no-name-d2f02182-d25d-4a6a-8d96-600d4dfa6652\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 01:02:16,079]\u001b[0m Trial 0 finished with value: -200.0 and parameters: {'n_steps': 438.1320239158379, 'gamma': 0.976888084210343, 'learning_rate': 0.008569495909565107, 'ent_coef': 1.6154532007233593e-07, 'cliprange': 0.1553591451650541, 'noptepochs': 1.5760136431055483, 'lam': 0.8285594283487052}. Best is trial 0 with value: -200.0.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 01:03:29,875]\u001b[0m Trial 1 finished with value: -200.0 and parameters: {'n_steps': 20.050674451352126, 'gamma': 0.9311514871252871, 'learning_rate': 1.8929888474605957e-05, 'ent_coef': 0.004526360224804509, 'cliprange': 0.2257727903029321, 'noptepochs': 3.1186497526629986, 'lam': 0.8980575036063343}. Best is trial 0 with value: -200.0.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 01:06:36,978]\u001b[0m Trial 3 finished with value: -200.0 and parameters: {'n_steps': 22.35401699014769, 'gamma': 0.9872203601975728, 'learning_rate': 0.00028540999663384504, 'ent_coef': 0.0028911392145500327, 'cliprange': 0.38358279172723686, 'noptepochs': 16.219713578599254, 'lam': 0.9704415308164384}. Best is trial 0 with value: -200.0.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 01:08:43,305]\u001b[0m Trial 4 finished with value: -200.0 and parameters: {'n_steps': 50.19215338102338, 'gamma': 0.9349562455004244, 'learning_rate': 0.0009294114629439143, 'ent_coef': 0.00518137812822939, 'cliprange': 0.2518777294239759, 'noptepochs': 39.73700280679408, 'lam': 0.9667701742123724}. Best is trial 0 with value: -200.0.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 01:09:03,045]\u001b[0m Trial 2 finished with value: -200.0 and parameters: {'n_steps': 31.239617323434025, 'gamma': 0.9752610101490984, 'learning_rate': 0.04098226397314594, 'ent_coef': 4.3167109121699056e-07, 'cliprange': 0.3175033941701547, 'noptepochs': 44.85435200465303, 'lam': 0.8561814135442241}. Best is trial 0 with value: -200.0.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6AJ0CDLcpOG"
      },
      "source": [
        "#Run lunarLander\r\n",
        "%tensorboard --logdir ./MountainCar/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2ZCEWk-cxQk"
      },
      "source": [
        "# Acrobot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bzMZFPPcwWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d677949c-d7ec-4b0e-b737-2521852f4ac8"
      },
      "source": [
        "def optimize_agent(trial):\r\n",
        "    \"\"\" Train the model and optimize\r\n",
        "        Optuna maximises the negative log likelihood, so we\r\n",
        "        need to negate the reward here\r\n",
        "    \"\"\"\r\n",
        "    # Create log dir\r\n",
        "    #log_dir = \"tmp/\"\r\n",
        "    #os.makedirs(log_dir, exist_ok=True)\r\n",
        "\r\n",
        "    #env_name='CartPole-v1'\r\n",
        "    #env_name='LunarLander-v2'\r\n",
        "    #env_name='MountainCar-v0'\r\n",
        "    env_name='Acrobot-v1'\r\n",
        "    model_params = optimize_ppo2(trial)\r\n",
        "    #env = make_vec_env(lambda: gym.make(env_name), n_envs=16, seed=0)\r\n",
        "    env = gym.make(env_name)\r\n",
        "    \r\n",
        "    model = PPO2('MlpPolicy', env, verbose=0, nminibatches=1, **model_params,\r\n",
        "                 tensorboard_log=\"./Acrobot/\")\r\n",
        "    #model = PPO('MlpPolicy', env, verbose=0, **model_params)\r\n",
        "    \r\n",
        "    time_steps=20000\r\n",
        "    model.learn(total_timesteps=int(time_steps))\r\n",
        "\r\n",
        "\r\n",
        "    mean_reward, _ = evaluate_policy(model, gym.make(env_name), n_eval_episodes=10)\r\n",
        "    #print(model.)\r\n",
        "    return mean_reward\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    study = optuna.create_study(direction=\"maximize\",pruner=optuna.pruners.SuccessiveHalvingPruner())\r\n",
        "    try:\r\n",
        "        study.optimize(optimize_agent, n_trials=5, n_jobs=4,)\r\n",
        "        #%tensorboard --logdir ./Acrobot/\r\n",
        "    except KeyboardInterrupt:\r\n",
        "        print('Interrupted by keyboard.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-14 01:09:34,879]\u001b[0m A new study created in memory with name: no-name-38d90f56-b893-4300-8046-114eb1a7a3c4\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 01:12:27,343]\u001b[0m Trial 1 finished with value: -85.5 and parameters: {'n_steps': 143.11399864802289, 'gamma': 0.9041878292971955, 'learning_rate': 0.0004399699396134245, 'ent_coef': 0.002270250650321917, 'cliprange': 0.16669138402086894, 'noptepochs': 8.177554024377828, 'lam': 0.8592372786661162}. Best is trial 1 with value: -85.5.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 01:12:31,620]\u001b[0m Trial 2 finished with value: -85.0 and parameters: {'n_steps': 80.2107402012829, 'gamma': 0.9576391291672102, 'learning_rate': 0.0010047547841189627, 'ent_coef': 7.897429165979982e-05, 'cliprange': 0.1960061897306572, 'noptepochs': 5.472860484783422, 'lam': 0.993380985270454}. Best is trial 2 with value: -85.0.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 01:12:31,885]\u001b[0m Trial 0 finished with value: -500.0 and parameters: {'n_steps': 1696.2915208698632, 'gamma': 0.9993149312049932, 'learning_rate': 2.0492263494566494e-05, 'ent_coef': 8.528670372863036e-06, 'cliprange': 0.3543075561027135, 'noptepochs': 9.594516033951592, 'lam': 0.8623241113452608}. Best is trial 2 with value: -85.0.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 01:12:48,187]\u001b[0m Trial 3 finished with value: -500.0 and parameters: {'n_steps': 83.28130630757447, 'gamma': 0.9917077493920017, 'learning_rate': 0.028494762546203437, 'ent_coef': 6.658839134205124e-07, 'cliprange': 0.335481991737149, 'noptepochs': 7.874026909404557, 'lam': 0.8865091502212621}. Best is trial 2 with value: -85.0.\u001b[0m\n",
            "\u001b[32m[I 2020-12-14 01:13:30,421]\u001b[0m Trial 4 finished with value: -272.9 and parameters: {'n_steps': 118.62985228437022, 'gamma': 0.9775384850761552, 'learning_rate': 2.6684365421704873e-05, 'ent_coef': 0.02369483439209054, 'cliprange': 0.25236940893548065, 'noptepochs': 3.7465230122653206, 'lam': 0.9092960909264349}. Best is trial 2 with value: -85.0.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e4-x6PPc2i-"
      },
      "source": [
        "#Run lunarLander\r\n",
        "%tensorboard --logdir ./Acrobot/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}